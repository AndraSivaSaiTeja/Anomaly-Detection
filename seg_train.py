# -*- coding: utf-8 -*-
"""new_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U8EOzooWsgS8SJBT0JZe8_-W6GKZKH1M
"""

from pathlib import Path
#from tqdm import tqdm
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import datetime
import argparse
#from str2bool import str2bool
import torch
import sys
from torch import optim
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
#from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms
#import segmentation_models_pytorch as smp

#upload these files dice_score.py,unet_model.py,unet_parts.py,dataset_cp.py,cutpaste.py from cutpaste code folder and mount drive
from dice_score import dice_loss
from dataset_cp import MVTecAT, Repeat
from dataset import MVTecDataset, get_data_transforms
from unet_model import UNet
from cutpaste import CutPasteNormal,CutPasteScar, CutPaste3Way, CutPasteUnion, cut_paste_collate_fn
from seg_eval import evaluation

def get_mask(normal,anomaly): #inputs are normal and artificial anomalous images (as batch)
    
    diff = (normal-anomaly).bool()

    return torch.logical_or(torch.logical_or(diff[:,0,:,:],diff[:,1,:,:]),diff[:,2,:,:]).int()

def run_training(data_type="screw",
                 model_dir="models",
                 epochs=256,
                 pretrained=True,
                 test_epochs=10,
                 freeze_resnet=20,
                 learninig_rate=0.03,
                 optim_name="SGD",
                 batch_size=64,
                 head_layer=8,
                 cutpate_type=CutPasteNormal,
                 device = "cuda",
                 workers=2, #default changed from 8 to 2
                 size = 256):
    torch.multiprocessing.freeze_support()
    #TODO: use script params for hyperparameter
    # Temperature Hyperparameter currently not used
    temperature = 0.2

    weight_decay = 0.00003
    momentum = 0.9
    #TODO: use f strings also for the date LOL
    model_name = f"model-" +data_type+ '-{date:%Y-%m-%d_%H_%M_%S}'.format(date=datetime.datetime.now() )

    #augmentation:
    min_scale = 1

    # create Training Dataset and Dataloader
    after_cutpaste_transform = transforms.Compose([])
    after_cutpaste_transform.transforms.append(transforms.ToTensor())
    #after_cutpaste_transform.transforms.append(transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                                    #std=[0.229, 0.224, 0.225])) 
                                                                    #removed for displaying proper anomaly images

    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])
    #to be normalized during training
    
    train_transform = transforms.Compose([])
    #train_transform.transforms.append(transforms.RandomResizedCrop(size, scale=(min_scale,1)))
    train_transform.transforms.append(transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1))
    # train_transform.transforms.append(transforms.GaussianBlur(int(size/10), sigma=(0.1,2.0)))
    train_transform.transforms.append(transforms.Resize((size,size)))
    train_transform.transforms.append(cutpate_type(transform = after_cutpaste_transform))
    # train_transform.transforms.append(transforms.ToTensor())

    train_data = MVTecAT("/home/ai22mtech11001/Siva/other/mvtec", data_type, transform = train_transform, size=int(size * (1/min_scale)))
    #train_data = MVTecAT("/home/ai22mtech11001/Siva/other/Data", data_type, transform = train_transform, size=int(size * (1/min_scale)))

    # Validation
    data_transform, gt_transform = get_data_transforms(256, 256)

    _class_ = data_type
    test_path = '/home/ai22mtech11001/Siva/other/mvtec/' + _class_
    test_data = MVTecDataset(root=test_path, transform=data_transform,
    gt_transform=gt_transform, phase="test")
    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False)
    
    #dataloader = dataloader.to(device)
    #print("data loader",len(dataloader))
    #dl = list(dataloader)
    #print(dl[0][0].shape,dl[0][1].shape,dl[0][2].shape) #all are batch_size*3*256*256

    #for displaying cutpaste anomaly images in dataloader
    # i = 0
    # j = 0
    # plt.imshow(dl[i][0][j].permute(1, 2, 0).numpy()) #Normal Image I
    # plt.show()
    # plt.imshow(dl[i][1][j].permute(1, 2, 0).numpy()) #CutPaste corresponding to I
    # plt.show()
    # plt.imshow(dl[i][2][j].permute(1, 2, 0).numpy()) #CutPaste Scar corresponding to I
    # plt.show()

    model = UNet(n_channels=3,n_classes=1) #output shape is (batch_size,classes,h,w)
    model = model.to(device)
    weights = torch.load("/home/ai22mtech11001/Siva/other/models/model-seg-unified-2023-06-17_12_56_32.tch")
    model.load_state_dict(weights)
    model.train()
                                                                    #input shape is (batch_size,in_channels,h,w)
    if optim_name == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=learninig_rate, momentum=momentum,  weight_decay=weight_decay)
        scheduler = CosineAnnealingWarmRestarts(optimizer, epochs)
        #scheduler = None
    elif optim_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learninig_rate, weight_decay=weight_decay)
        scheduler = None
    else:
        print(f"ERROR unkown optimizer: {optim_name}")

    criterion = torch.nn.BCEWithLogitsLoss()

    # test_transform = transforms.Compose([])
    # test_transform.transforms.append(transforms.Resize((size,size)))
    # test_transform.transforms.append(transforms.ToTensor())
    # test_image = Image.open("/content/drive/MyDrive/mvtec/bottle/test/broken_large/000.png")
    # test_image = test_transform(test_image) #3*256*256
    # test_image = test_image.unsqueeze(dim=0)
    # test_gt = Image.open("/content/drive/MyDrive/mvtec/bottle/ground_truth/broken_large/000_mask.png")
    # test_gt = test_transform(test_gt) #1*256*256
    # test_gt = test_gt.squeeze() #256*256


    losses = []
    for i in range(1,epochs+1):
      
      model.train()
      epoch_loss = 0
      test_true = 0
      test_pred = 0
      dataloader = DataLoader(Repeat(train_data, 240), batch_size=batch_size, drop_last=True,
                            shuffle=True, num_workers=workers, collate_fn=cut_paste_collate_fn,
                            persistent_workers=True, pin_memory=True, prefetch_factor=5)
      for j,(normal,cp,cps) in enumerate(dataloader):
        if(True):
            inputs = torch.concat((normal,cp,cps),dim=0)
        else:
            inputs = torch.concat((normal,normal,normal),dim=0)

        true_masks = get_mask(inputs,torch.concat((normal,normal,normal),dim=0))
        #print(true_masks.shape)
        #for displaying mask
        # plt.imshow(inputs[0].permute(1, 2, 0).numpy())
        # plt.show()
        # plt.imshow(true_masks[0].numpy())
        # plt.show()

        inputs = inputs.to(device)
        outputs = model(inputs)
        #outputs = torch.nn.functional.sigmoid(outputs)

        #testing
        #pred = model(normalize(test_image).to(device))
        #pred = torch.nn.functional.sigmoid(pred) #1*256*256

        # if(j==0):
        #   test_true = true_masks[0].numpy()
        #   test_pred = outputs[0][0].cpu().detach().numpy()
        #print(outputs.shape)
        # print(outputs.shape)
        loss = criterion(outputs.squeeze(),true_masks.float().to(device))
        loss += dice_loss(torch.nn.functional.sigmoid(outputs.squeeze()), true_masks.float().to(device), multiclass=False)
        epoch_loss += loss
        optimizer.zero_grad()
        loss.backward()
        #print(loss)
        optimizer.step()
      #plt.imshow(np.concatenate((test_gt.numpy(),pred.squeeze().cpu().detach().numpy()),axis=1))
      #plt.imshow(pred.squeeze().cpu().detach().numpy())
      #plt.show()
      print("Epoch [%i/%i]: loss %.4f"%(i,epochs,epoch_loss))
      losses.append(epoch_loss.item())
      if (i + 1) % 1 == 0:
            auroc_px, auroc_sp, aupro_px = evaluation(model, test_dataloader, device)
            print('Pixel Auroc:{:.3f}, Sample Auroc{:.3f}, Pixel Aupro{:.3}'.format(auroc_px, auroc_sp, aupro_px))
      torch.save(model.state_dict(), model_dir + f"/{model_name}.tch")

    #   plt.plot(losses)
    #   plt.savefig("plots/epoch_loss_plot_"+str(i)+".png")
    #   plt.close()

all_types = ['bottle',
             'cable',
             'capsule',
             'carpet',
             'grid',
             'hazelnut',
             'leather',
             'metal_nut',
             'pill',
             'screw',
             'tile',
             'toothbrush',
             'transistor',
             'wood',
             'zipper']

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"using device: {device}")
variant_map = {'normal':CutPasteNormal, 'scar':CutPasteScar, '3way':CutPaste3Way, 'union':CutPasteUnion}
variant = variant_map['3way'] #using 3way

for data_type in ['bottle']:#types:
    print(f"training {data_type}")
    Path("models").mkdir(exist_ok=True, parents=True)
    run_training(data_type,
                  model_dir="/home/ai22mtech11001/Siva/other/models",
                  epochs=200,
                  pretrained=True,
                  test_epochs=10,
                  freeze_resnet=20,
                  learninig_rate=1e-4,
                  optim_name='adam',
                  batch_size=16,
                  head_layer=1,
                  device=device,
                  cutpate_type=variant,
                  workers=2)

# size = 256
# test_transform = transforms.Compose([])
# test_transform.transforms.append(transforms.Resize((size,size)))
# test_transform.transforms.append(transforms.ToTensor())
# #test_transform.transforms.append(transforms.Normalize(mean=[0.485, 0.456, 0.406],
#  #                                                   std=[0.229, 0.224, 0.225]))
# test_data_eval = MVTecAT("/content/drive/MyDrive/mvtec", "bottle", size, transform = test_transform, mode="test")

# dataloader_test = DataLoader(test_data_eval, batch_size=1,
#                             shuffle=False, num_workers=0)

# model = UNet(n_channels=3,n_classes=1) #output shape is (batch_size,classes,h,w)
# device = "cuda" if torch.cuda.is_available() else "cpu"
# model = model.to(device)
# model.load_state_dict(torch.load("/content/models/model-bottle-2023-05-30_15_07_06.tch"))

# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])
# for x,label in dataloader_test:
#   plt.imshow(model(normalize(x).to(device)).detach().squeeze().cpu().numpy())
#   plt.show()
#   plt.imshow(x.squeeze().permute(1,2,0).numpy())
#   plt.show()
